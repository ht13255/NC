# app.py - ì‚¬ìš©ì ì—…ë¡œë“œ ì†Œì„¤ í•™ìŠµ & ìŠ¤íƒ€ì¼ ì¬ìƒì„± Streamlit ì•±
import os
import torch
import multiprocessing
import streamlit as st
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    TextDataset,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training

# ìµœëŒ€ CPU ìŠ¤ë ˆë“œ ì‚¬ìš©
NUM_THREADS = multiprocessing.cpu_count()
torch.set_num_threads(NUM_THREADS)

def fine_tune(model_name, train_file, output_dir, epochs, batch_size):
    # LoRAë¥¼ ì´ìš©í•œ ë¯¸ì„¸ì¡°ì •
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_8bit=False,  # CPUì—ì„œëŠ” False
        device_map={"": "cpu"}
    )
    model = prepare_model_for_int8_training(model)
    peft_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, peft_config)

    dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=train_file,
        block_size=512
    )
    collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        num_train_epochs=epochs,
        logging_steps=50,
        save_steps=200,
        save_total_limit=1,
        fp16=False,
        dataloader_num_workers=NUM_THREADS
    )
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=dataset,
        data_collator=collator
    )
    trainer.train()
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)


def generate_chunk(model_dir, prompt, n_tokens, temperature=0.8):
    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_dir, device_map={"": "cpu"})
    inputs = tokenizer(prompt, return_tensors='pt')
    generated = model.generate(
        **inputs,
        max_new_tokens=n_tokens,
        do_sample=True,
        temperature=temperature,
        top_p=0.9,
        top_k=50
    )
    return tokenizer.decode(generated[0], skip_special_tokens=True)

# Streamlit ë ˆì´ì•„ì›ƒ
st.set_page_config(page_title="Fine-Tune Novel Generator", layout="wide")
st.title("ğŸ“š ìŠ¤íƒ€ì¼ ê¸°ë°˜ ì†Œì„¤ ìƒì„±ê¸°")

# 1) í•™ìŠµ íŒŒíŠ¸
st.sidebar.header("1. ì†Œì„¤ í•™ìŠµ")
uploaded = st.sidebar.file_uploader("í•™ìŠµí•  ì†Œì„¤(.txt) ì—…ë¡œë“œ", type="txt")
model_name = st.sidebar.text_input("ê¸°ë³¸ ëª¨ë¸ëª…", value="mistralai/Mistral-7B-Instruct")
epochs = st.sidebar.number_input("Epochs", min_value=1, max_value=5, value=3)
batch_size = st.sidebar.number_input("Batch size", min_value=1, max_value=4, value=1)
output_dir = st.sidebar.text_input("ì¶œë ¥ ë””ë ‰í† ë¦¬", value="./fine-tuned-model")
if uploaded:
    train_path = os.path.join(".", "train.txt")
    with open(train_path, "wb") as f:
        f.write(uploaded.getbuffer())
    st.sidebar.success("ì—…ë¡œë“œ ì™„ë£Œ")
    if st.sidebar.button("í•™ìŠµ ì‹œì‘"):
        with st.spinner("ëª¨ë¸ í•™ìŠµ ì¤‘... (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤) ğŸŒ±"):
            fine_tune(model_name, train_path, output_dir, epochs, batch_size)
        st.sidebar.success("í•™ìŠµ ì™„ë£Œ! ğŸ‰")
        st.balloons()

# 2) ìƒì„± íŒŒíŠ¸
st.header("2. ì†Œì„¤ ìƒì„±")
prompt = st.text_area("ì†Œì„¤ ì‹œì‘ ë¬¸ì¥ ë˜ëŠ” ì¤„ê±°ë¦¬ ì…ë ¥", height=150)
length_opt = st.selectbox("ì›í•˜ëŠ” ê¸¸ì´", ["ë‹¨í¸(128)", "ì¤‘í¸(512)", "ì¥í¸(1024)"])
total_tokens = int(length_opt.split("(")[1].strip(")"))
chunk_size = st.number_input("ì²­í¬ í¬ê¸°(í† í°)", value=128, step=64, min_value=32)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'generated_text' not in st.session_state:
    st.session_state.generated_text = ""
if 'tokens_generated' not in st.session_state:
    st.session_state.tokens_generated = 0

output_box = st.empty()

# ìƒì„± ì‹œì‘
if st.button("ìƒì„± ì‹œì‘"):
    st.session_state.generated_text = ""
    st.session_state.tokens_generated = 0
    # ì‚¬ìš©ìê°€ í•™ìŠµí–ˆë‹¤ë©´ output_dirë¡œ, ì•„ë‹ˆë©´ ê¸°ë³¸ ëª¨ë¸
    model_dir = output_dir if os.path.isdir(output_dir) else model_name
    # ì²« ì²­í¬
    n = min(chunk_size, total_tokens)
    result = generate_chunk(model_dir, prompt, n)
    st.session_state.generated_text = result
    st.session_state.tokens_generated = n

# ê³„ì† ìƒì„±
if st.session_state.tokens_generated < total_tokens:
    if st.button("ê³„ì† ìƒì„±"):
        model_dir = output_dir if os.path.isdir(output_dir) else model_name
        remaining = total_tokens - st.session_state.tokens_generated
        n = min(chunk_size, remaining)
        result = generate_chunk(model_dir, st.session_state.generated_text, n)
        st.session_state.generated_text += result
        st.session_state.tokens_generated += n

# ê²°ê³¼ í‘œì‹œ
if st.session_state.generated_text:
    output_box.markdown("### ìƒì„±ëœ ì†Œì„¤")
    output_box.write(st.session_state.generated_text)

