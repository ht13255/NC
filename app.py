# app.py - BitNet ìŠ¤íƒ€ì¼ ì†Œì„¤ íŒŒì¸íŠœë‹ & ìƒì„± Streamlit ì•±
import os
import subprocess
import torch
import multiprocessing
import streamlit as st
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    TextDataset,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training

# ìµœëŒ€ CPU ìŠ¤ë ˆë“œ ì„¤ì •
NUM_THREADS = multiprocessing.cpu_count()
torch.set_num_threads(NUM_THREADS)

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="BitNet ìŠ¤íƒ€ì¼ ì†Œì„¤ ìƒì„±ê¸°", layout="wide")
st.title("ğŸ“– BitNet ìŠ¤íƒ€ì¼ ì†Œì„¤ íŒŒì¸íŠœë‹ & ìƒì„±ê¸°")

# --- 1ï¸âƒ£ ì†Œì„¤ ìŠ¤íƒ€ì¼ í•™ìŠµ ì˜ì—­ ---
st.header("1ï¸âƒ£ ì†Œì„¤ ìŠ¤íƒ€ì¼ í•™ìŠµ")
# íŒŒì¼ ì—…ë¡œë” (ë©”ì¸ ì˜ì—­)
uploaded = st.file_uploader("í•™ìŠµìš© ì†Œì„¤(.txt) ì—…ë¡œë“œ", type=["txt"])
# íŒŒì¸íŠœë‹ íŒŒë¼ë¯¸í„°
model_name = st.text_input("ê¸°ë³¸ ëª¨ë¸ëª… (Hugging Face)", value="mistralai/Mistral-7B-Instruct")
epochs = st.number_input("Epochs", min_value=1, max_value=5, value=3)
batch_size = st.number_input("Batch size", min_value=1, max_value=4, value=1)
output_dir = st.text_input("Fine-tuned ëª¨ë¸ ì €ì¥ ê²½ë¡œ", value="./fine_tuned")
convert = st.checkbox("BitNetìš© GGUFë¡œ ë³€í™˜", value=True)

# í•™ìŠµ ë° ë³€í™˜ í•¨ìˆ˜ ì •ì˜
def fine_tune_and_convert():
    train_path = os.path.join(".", "train.txt")
    with open(train_path, "wb") as f:
        f.write(uploaded.getbuffer())
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=False, device_map={"": "cpu"})
    model = prepare_model_for_int8_training(model)
    peft_config = LoraConfig(r=8, lora_alpha=32, target_modules=["q_proj","v_proj"], lora_dropout=0.05, bias="none", task_type="CAUSAL_LM")
    model = get_peft_model(model, peft_config)
    dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=512)
    collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    args = TrainingArguments(output_dir=output_dir, per_device_train_batch_size=batch_size,
                              num_train_epochs=epochs, logging_steps=50, save_total_limit=1,
                              fp16=False, dataloader_num_workers=NUM_THREADS)
    trainer = Trainer(model=model, args=args, train_dataset=dataset, data_collator=collator)
    trainer.train()
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    st.success("âœ… í•™ìŠµ ì™„ë£Œ: ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    if convert:
        gguf_path = os.path.join("models", "finetuned.gguf")
        os.makedirs(os.path.dirname(gguf_path), exist_ok=True)
        cmd = ["python", "bitnet.cpp/3rdparty/llama.cpp/convert_hf_to_gguf.py",
               "--model-dir", output_dir, "--outfile", gguf_path]
        subprocess.run(cmd, check=True)
        st.success(f"âœ… GGUF ë³€í™˜ ì™„ë£Œ: {gguf_path}")

# í•™ìŠµ íŠ¸ë¦¬ê±° ë²„íŠ¼
if uploaded and st.button("í•™ìŠµ ì‹œì‘"):  
    with st.spinner("ëª¨ë¸ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):  
        fine_tune_and_convert()
        st.balloons()

# --- 2ï¸âƒ£ ì†Œì„¤ ìƒì„± ì˜ì—­ ---
st.header("2ï¸âƒ£ ì†Œì„¤ ìƒì„±")
outline = st.text_area("ì¤„ê±°ë¦¬ ë˜ëŠ” ì „ì²´ ë‚´ìš© ìš”ì•½ ì…ë ¥", height=120)
length = st.selectbox("ìƒì„± ê¸¸ì´ (í† í°)", [128, 512, 1024])
chunk = st.number_input("ì²­í¬ í¬ê¸° (í† í°)", value=128, step=64, min_value=32)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'gen_text' not in st.session_state:
    st.session_state.gen_text = ""
if 'gen_tokens' not in st.session_state:
    st.session_state.gen_tokens = 0

# ì¶”ë¡  í•¨ìˆ˜
def infer_chunk(prompt, n):
    gguf_path = "models/finetuned.gguf" if os.path.exists("models/finetuned.gguf") else None
    model_arg = ["-m", gguf_path] if gguf_path else ["-m", "models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf"]
    cmd = ["python", "bitnet.cpp/run_inference.py"] + model_arg + ["-p", prompt, "-n", str(n), "-t", str(NUM_THREADS), "-temp", "0.8"]
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    out = ""
    for line in proc.stdout:
        out += line
    proc.wait()
    return out

# ìƒì„± ì‹œì‘ ë²„íŠ¼
if st.button("ìƒì„± ì‹œì‘"):
    st.session_state.gen_text = outline
    st.session_state.gen_tokens = 0
    first_n = min(chunk, length)
    res = infer_chunk(st.session_state.gen_text, first_n)
    st.session_state.gen_text += res
    st.session_state.gen_tokens += first_n

# ê³„ì† ìƒì„± ë²„íŠ¼
if st.session_state.gen_tokens < length:
    if st.button("ê³„ì† ìƒì„±"):
        remaining = length - st.session_state.gen_tokens
        next_n = min(chunk, remaining)
        res = infer_chunk(st.session_state.gen_text, next_n)
        st.session_state.gen_text += res
        st.session_state.gen_tokens += next_n

# ê²°ê³¼ í‘œì‹œ
if st.session_state.gen_text:
    st.markdown("### ìƒì„±ëœ ì†Œì„¤")
    st.write(st.session_state.gen_text)

